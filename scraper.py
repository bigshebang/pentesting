#!/usr/bin/env python
# Luke Matarazzo
# Pen Testing - Email Scraper

from urllib2 import urlopen, urlparse
import re
import argparse
from BeautifulSoup import BeautifulSoup, SoupStrainer
from sys import exit

# emailRegex

# def getEmails(content):
# 	print "getEmails method"

# def getLinks(content):
# 	print "getLinks method"

def isValidLink(baseUrl,newUrl):
	return 1

#create the email regex object once for efficiency
# emailRegex = re.compile("[\w]+@[a-zA-Z0-9-.]+\.[a-zA-Z]+")
emailRegex = re.compile("[\w\d._%+-]+@[\w\d.-]+\.\w+")

def scrape(url):
	"""
	Scrape a web page for links and emails.

	Purpose: Download a given URL's HTML and parse it for links and emails. All links added to the
			 list will be in the same domain as the given url.
	Parameters: url - the URL that is going to be scraped
	Returns: A dictionary containing two separate lists that consist of links and emails.
	"""
	#visit URL given
	try:
		result = urlopen(url).read()
	except:
		print "Invalid URL: " + url
		exit(-1)

	if not result:
		print "It seems there was nothing at the given URL."
		exit(0)

	# print result
	print "result is"
	print result

	scrapeLinks = []
	scrapeEmails = []
	#get all the links on the page
	for link in BeautifulSoup(result, parseOnlyThese=SoupStrainer("a")):
		if link.has_key("href"):
			linkRef = str(link["href"])
			if isValidLink(url, linkRef):
				scrapeLinks.append(linkRef)

	# scrapeEmails = emailRegex.findall(result)
	# with open("temp.txt", "w") as f:
		# f.write(BeautifulSoup(result).prettify())
	scrapeEmails = emailRegex.findall(result)

	# scrapeLinks = getLinks(result)
	# scrapeEmails = getEmails(result)
	return {"links": scrapeLinks, "emails": scrapeEmails}

def main():
	#parser stuff
	parser = argparse.ArgumentParser()
	parser.add_argument("url", help="This is the URL that is to be scraped")
	parser.add_argument("-n", "--number-of-emails", "--emails", type=int, default=20,
						help="Number of emails before the scraper stops")
	parser.add_argument("-f", "--file", default="emails.txt",
						help="Output file that will hold the emails")
	parser.add_argument("-d", "--delimiter", default="\n",
						help="Delimiter for the output email file. Default is a newline")
	args = parser.parse_args()

	#make sure it starts with http:// or https://, otherwise add http://
	if args.url[:7] != "http://" and args.url[:8] != "https://":
		args.url = "http://" + args.url

	

	#get the urls and emails from the first URL
	firstPage = scrape(args.url)
	print "links are:"
	print firstPage["links"]
	print "\n\nemails are:"
	print firstPage["emails"]

	#loop through all given links
	# for link in firstPage["links"]:
	# 	tempPage = scrape(link)


if __name__ == "__main__":
	main()
